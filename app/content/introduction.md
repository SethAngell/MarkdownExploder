# Introduction

Mixed Reality creeps more into conversation with each passing day, with systems such as Meta's (previously Facebook) Metaverse becoming a commonplace topic. Probably one of the most notable instances of it within popular culture would be the explosion of the Augmented Reality application _Pokemon Go_ in 2016. However, the uses of mixed reality extend far outside the social sphere. With over half a billion downloads[^PGoStat] the application has no doubt been a success, so much so that it has piqued the interest of educators. A French study found that by integrating experiences similar to Pokemon Go, students were able to connect more deeply with the subject they were currently studying. [^PGoStudy] 

There are 4 widely accepted methods to how people learn: Visual, Auditory, Kinesthetic, and Reading/Writing. [^4TypesOfLearning] Due to environmental, geographic, or a litany of other potential road blocks, some students may find that they're restricted to only one of these teaching types even though they may excel in others. Especially at advanced levels, instructional material for concepts such as molecular chemistry and orbital mechanics can become incredibly difficult to find in forms other than written word or spoken lectures. With the advent of entirely virtual realities, students could step into these previously inaccessible environments. Making the small big and the big small are only one of the areas where mixed reality excels. As the tools for creation improve, so will the opportunity for use. 

Labs all across the world are working on novel experiences taking advantage of this new technology, including UNCW's very own Mixed Reality lab. Simulations of how to treat heat exhaustion and teach school children about sea turtles are just two of the educational experiences being developed in VR. Extending from that, the lab develops therapeutic virtual environments to help former smokers cope with common triggers, and data visualization tools to help aid projects at Oak Ridge National Labs. The possibilities are limited only by development time, imagination, and the tools at hand.

The biggest catalysts for these advancements have been the release of powerful game engine's such as _Unity3D_ and _The Unreal Engine_ for free use, and the development of headsets such as the _Oculus Quest_ and _HTC Vive_ for sale at consumer price points. Whereas previously these experiences and the dev tools needed to create them were reserved only for research labs and AAA game studios, now the average person with a laptop and a small initial investment (~$300) can have access to the same tools as big studios. This change has been instrumental to the space. This explosion can be easily identified within the Google Search Frequency results for the term "Mixed Reality" from 2010 to date. Much like the introduction of the personal computer, citizens now have access to not just the technology required to experience these simulations, but also the tools required to develop them.

![A graph representing the Frequency in which users search the term "Mixed Reality" in the google search engine. Occurrence is relatively low on the scale (0-10) until 2017 when it skyrockets to 100](assets/MRGraph.png)[^MRStats]

That being said, the internet wasn't created with systems in place for Netflix to exist. Those needed to be built overtime. As new technology progresses, new problems will need to be solved. One of which has been regularly encountered in the Mixed Reality Lab here at UNCW. While many types of virtual experiences are possible with these tools, the majority of open source tools on the market are geared towards the entertainment and gaming sector of VR. While these can usually be retrofitted to work in the educational space, some are so gaming centric that the development time required to make them work isn't worth it. Likewise, the development time required to create the feature itself is oftentimes substantial as well, especially for one-off projects. One great example of this issue has been synchronous transfer of video and textual data between the headset and an external service. Through the use of HTTP requests and some data structures work, we've developed a system for efficiently loading and displaying photo assets are runtime without crashing the program. However, we've yet to find a solution for streaming an in-game camera in a way that wouldn't require bespoke solutions for each of our projects. 

That leads to my proposal: A simple two part system which enables the easy creation of a WebRTC stream and WebSocket (Socket.IO flavored) connection from a unity client, and a backend system for ingesting said stream as well as broadcasting realtime messages from an external user to all subscribed unity clients. This project would be built in a modern, containerized, micro-service architecture to enable rapid deployment as well as the utilization of any of the core features as a standalone component. The creation of this system would allow for multiple team's to integrate realtime streaming of either the player camera or another in game device to external sources, as well as communication with users inside of the headsets. A commonly requested, yet rarely delivered feature request.
            
            
[^PGoStat]: https://sensortower.com/blog/pokemon-go-five-billion-revenue
[^PGoStudy]: https://dl.acm.org/doi/10.1145/3110292.3110293 
[^4TypesOfLearning]: https://bau.edu/blog/types-of-learning-styles/
[^MRStats]: https://trends.google.com/trends/explore?date=all&geo=US&q=Mixed%20Reality
