# Introduction

This whole project was made possible by John Gruber's early work on the Markdown spec. Without that, I would probably have to write a worse version of it myself.

Mixed Reality creeps more and more into daily conversation as time progresses, with systems such as Meta's (previously Facebook) Metaverse becoming a commonplace topic, but the uses of mixed reality experiences extend far outside of the social sphere. Probably one of the most notable instances of it within popular culture would be the explosion of the Augmented Reality application _Pokemon Go_ in 2016. With over half a billion downloads[^PGoStat] the application has no doubt been a success, so much so that it has piqued the interest of educators. A French study found that by integrating experiences similar to the Pokemon Go application, students were able to connect more deeply with the subject they were currently studying. [^PGoStudy] There are 4 widely accepted methods to how people learn: Visual, Auditory, Kinesthetic, and Reading/Writing. [^4TypesOfLearning] Due to environmental, geographic, or other restrictions, some students may find that they're restricted to only one of these teaching types even though they may excel in others. Especially at advanced levels, concepts such as molecular chemistry and orbital mechanics can become incredibly difficult to learn outside of written word or lectures. With the advent of entirely virtual realities, students could step into these previously inaccessible environments. Making the small big and the big small are only one of the areas where mixed reality excels, likewise with education. As the tools for creation improve, so will the opportunity for use. 

Labs all across the world are working on novel experiences taking advantage of this new technology, including UNCW's very own Mixed Reality lab. Simulations of how to treat heat exhaustion and teach school children about sea turtles are just two of the educational experiences being developed in VR. Extending from that, the lab develops therapeutic virtual environments to help former smokers cope with common triggers, and data visualization tools to help aid projects at Oak Ridge National Labs. The possibilities are limited only by development time, imagination, and the tools at hand.

The biggest catalysts for these advancements have been the release of powerful game engine's such as _Unity3D_ and _Unreal_ for free use, and the development of headsets such as the _Oculus Quest_ and _HTC Vive_ for sale at consumer price points. Whereas previously these experiences and the dev tools needed to create them where reserved only for research labs and AAA game studios, now any average person with a laptop and a small initial investment (~$300) can have access to the same tools as big studios. This change has been instrumental to the space. Much like the introduction of the personal computer, citizens now have access to not just the technology required to experience these simulations, but also the tools required to develop them.


That being said, the internet didn't start with capability to build Netflix. As new technology progresses, new problems will need to be solved. One of which has bene regularly encountered in the Mixed Reality Lab here at UNCW. While many types of virtual experiences are possible with these tools, the majority of open source tools on the market are geared towards the entertainment and gaming sector of VR. While these can usually be retrofitted to work in the educational space, some are so gaming centric that the development time required to make them work isn't worth it. However, this doesn't mean that the development time required to create the feature itself is minimal either. One great example of this issue has been synchronous transfer of video and textual data between the headset and an external service. Through the use of HTTP requests and some data structures work, we've developed a system for efficiently loading and displaying photo assets are runtime without crashing the program. However, we've yet to find a solution for streaming an in-game camera in a way that wouldn't require bespoke solutions for each of our projects. 

That leads to my proposal: A simple two part system which enables the easy creation of a WebRTC stream and Realtime database subscription from a unity client, and a backend system for ingesting said stream as well as broadcasting realtime messages from a database to all subscribed unity clients. This project would be built in a modern, containerized, micro-service architecture to enable rapid deployment as well as the utilization of any of the core features as a standalone component. The creation of this system would allow for multiple team's to integrate realtime streaming of either the player camera or another in game device to external sources, as well as communication with users inside of the headsets - a commonly requested, yet rarely delivered feature request.
            
            
[^PGoStat]: https://sensortower.com/blog/pokemon-go-five-billion-revenue
[^PGoStudy]: https://dl.acm.org/doi/10.1145/3110292.3110293 
[^4TypesOfLearning]: https://bau.edu/blog/types-of-learning-styles/
